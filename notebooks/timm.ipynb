{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"resnet18\", pretrained=True)\n",
    "model.reset_classifier(0, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2          [1, 64, 112, 112]             128\n",
      "              ReLU-3          [1, 64, 112, 112]               0\n",
      "         MaxPool2d-4            [1, 64, 56, 56]               0\n",
      "            Conv2d-5            [1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6            [1, 64, 56, 56]             128\n",
      "          Identity-7            [1, 64, 56, 56]               0\n",
      "              ReLU-8            [1, 64, 56, 56]               0\n",
      "          Identity-9            [1, 64, 56, 56]               0\n",
      "           Conv2d-10            [1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-11            [1, 64, 56, 56]             128\n",
      "             ReLU-12            [1, 64, 56, 56]               0\n",
      "       BasicBlock-13            [1, 64, 56, 56]               0\n",
      "           Conv2d-14            [1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-15            [1, 64, 56, 56]             128\n",
      "         Identity-16            [1, 64, 56, 56]               0\n",
      "             ReLU-17            [1, 64, 56, 56]               0\n",
      "         Identity-18            [1, 64, 56, 56]               0\n",
      "           Conv2d-19            [1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-20            [1, 64, 56, 56]             128\n",
      "             ReLU-21            [1, 64, 56, 56]               0\n",
      "       BasicBlock-22            [1, 64, 56, 56]               0\n",
      "           Conv2d-23           [1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-24           [1, 128, 28, 28]             256\n",
      "         Identity-25           [1, 128, 28, 28]               0\n",
      "             ReLU-26           [1, 128, 28, 28]               0\n",
      "         Identity-27           [1, 128, 28, 28]               0\n",
      "           Conv2d-28           [1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29           [1, 128, 28, 28]             256\n",
      "           Conv2d-30           [1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-31           [1, 128, 28, 28]             256\n",
      "             ReLU-32           [1, 128, 28, 28]               0\n",
      "       BasicBlock-33           [1, 128, 28, 28]               0\n",
      "           Conv2d-34           [1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-35           [1, 128, 28, 28]             256\n",
      "         Identity-36           [1, 128, 28, 28]               0\n",
      "             ReLU-37           [1, 128, 28, 28]               0\n",
      "         Identity-38           [1, 128, 28, 28]               0\n",
      "           Conv2d-39           [1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-40           [1, 128, 28, 28]             256\n",
      "             ReLU-41           [1, 128, 28, 28]               0\n",
      "       BasicBlock-42           [1, 128, 28, 28]               0\n",
      "           Conv2d-43           [1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-44           [1, 256, 14, 14]             512\n",
      "         Identity-45           [1, 256, 14, 14]               0\n",
      "             ReLU-46           [1, 256, 14, 14]               0\n",
      "         Identity-47           [1, 256, 14, 14]               0\n",
      "           Conv2d-48           [1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-49           [1, 256, 14, 14]             512\n",
      "           Conv2d-50           [1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-51           [1, 256, 14, 14]             512\n",
      "             ReLU-52           [1, 256, 14, 14]               0\n",
      "       BasicBlock-53           [1, 256, 14, 14]               0\n",
      "           Conv2d-54           [1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-55           [1, 256, 14, 14]             512\n",
      "         Identity-56           [1, 256, 14, 14]               0\n",
      "             ReLU-57           [1, 256, 14, 14]               0\n",
      "         Identity-58           [1, 256, 14, 14]               0\n",
      "           Conv2d-59           [1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-60           [1, 256, 14, 14]             512\n",
      "             ReLU-61           [1, 256, 14, 14]               0\n",
      "       BasicBlock-62           [1, 256, 14, 14]               0\n",
      "           Conv2d-63             [1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-64             [1, 512, 7, 7]           1,024\n",
      "         Identity-65             [1, 512, 7, 7]               0\n",
      "             ReLU-66             [1, 512, 7, 7]               0\n",
      "         Identity-67             [1, 512, 7, 7]               0\n",
      "           Conv2d-68             [1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-69             [1, 512, 7, 7]           1,024\n",
      "           Conv2d-70             [1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-71             [1, 512, 7, 7]           1,024\n",
      "             ReLU-72             [1, 512, 7, 7]               0\n",
      "       BasicBlock-73             [1, 512, 7, 7]               0\n",
      "           Conv2d-74             [1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-75             [1, 512, 7, 7]           1,024\n",
      "         Identity-76             [1, 512, 7, 7]               0\n",
      "             ReLU-77             [1, 512, 7, 7]               0\n",
      "         Identity-78             [1, 512, 7, 7]               0\n",
      "           Conv2d-79             [1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-80             [1, 512, 7, 7]           1,024\n",
      "             ReLU-81             [1, 512, 7, 7]               0\n",
      "       BasicBlock-82             [1, 512, 7, 7]               0\n",
      "         Identity-83             [1, 512, 7, 7]               0\n",
      "         Identity-84             [1, 512, 7, 7]               0\n",
      "SelectAdaptivePool2d-85             [1, 512, 7, 7]               0\n",
      "         Identity-86             [1, 512, 7, 7]               0\n",
      "================================================================\n",
      "Total params: 11,176,512\n",
      "Trainable params: 11,176,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 75.03\n",
      "Params size (MB): 42.64\n",
      "Estimated Total Size (MB): 118.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3,224,224), batch_size=1, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(x)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_hat\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(x)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_hat\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Work/github/IgorJanos/nndemo/.venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Work/github/IgorJanos/nndemo/.venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = torch.randn(size=(2,3,224,224))\n",
    "y_hat = model(x)\n",
    "print(y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"vit_base_patch16_224.augreg2_in21k_ft_in1k\", pretrained=True)\n",
    "model.reset_classifier(0, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [1, 768, 14, 14]         590,592\n",
      "          Identity-2              [1, 196, 768]               0\n",
      "        PatchEmbed-3              [1, 196, 768]               0\n",
      "           Dropout-4              [1, 197, 768]               0\n",
      "          Identity-5              [1, 197, 768]               0\n",
      "          Identity-6              [1, 197, 768]               0\n",
      "         LayerNorm-7              [1, 197, 768]           1,536\n",
      "            Linear-8             [1, 197, 2304]       1,771,776\n",
      "          Identity-9           [1, 12, 197, 64]               0\n",
      "         Identity-10           [1, 12, 197, 64]               0\n",
      "           Linear-11              [1, 197, 768]         590,592\n",
      "          Dropout-12              [1, 197, 768]               0\n",
      "        Attention-13              [1, 197, 768]               0\n",
      "         Identity-14              [1, 197, 768]               0\n",
      "         Identity-15              [1, 197, 768]               0\n",
      "        LayerNorm-16              [1, 197, 768]           1,536\n",
      "           Linear-17             [1, 197, 3072]       2,362,368\n",
      "             GELU-18             [1, 197, 3072]               0\n",
      "          Dropout-19             [1, 197, 3072]               0\n",
      "         Identity-20             [1, 197, 3072]               0\n",
      "           Linear-21              [1, 197, 768]       2,360,064\n",
      "          Dropout-22              [1, 197, 768]               0\n",
      "              Mlp-23              [1, 197, 768]               0\n",
      "         Identity-24              [1, 197, 768]               0\n",
      "         Identity-25              [1, 197, 768]               0\n",
      "            Block-26              [1, 197, 768]               0\n",
      "        LayerNorm-27              [1, 197, 768]           1,536\n",
      "           Linear-28             [1, 197, 2304]       1,771,776\n",
      "         Identity-29           [1, 12, 197, 64]               0\n",
      "         Identity-30           [1, 12, 197, 64]               0\n",
      "           Linear-31              [1, 197, 768]         590,592\n",
      "          Dropout-32              [1, 197, 768]               0\n",
      "        Attention-33              [1, 197, 768]               0\n",
      "         Identity-34              [1, 197, 768]               0\n",
      "         Identity-35              [1, 197, 768]               0\n",
      "        LayerNorm-36              [1, 197, 768]           1,536\n",
      "           Linear-37             [1, 197, 3072]       2,362,368\n",
      "             GELU-38             [1, 197, 3072]               0\n",
      "          Dropout-39             [1, 197, 3072]               0\n",
      "         Identity-40             [1, 197, 3072]               0\n",
      "           Linear-41              [1, 197, 768]       2,360,064\n",
      "          Dropout-42              [1, 197, 768]               0\n",
      "              Mlp-43              [1, 197, 768]               0\n",
      "         Identity-44              [1, 197, 768]               0\n",
      "         Identity-45              [1, 197, 768]               0\n",
      "            Block-46              [1, 197, 768]               0\n",
      "        LayerNorm-47              [1, 197, 768]           1,536\n",
      "           Linear-48             [1, 197, 2304]       1,771,776\n",
      "         Identity-49           [1, 12, 197, 64]               0\n",
      "         Identity-50           [1, 12, 197, 64]               0\n",
      "           Linear-51              [1, 197, 768]         590,592\n",
      "          Dropout-52              [1, 197, 768]               0\n",
      "        Attention-53              [1, 197, 768]               0\n",
      "         Identity-54              [1, 197, 768]               0\n",
      "         Identity-55              [1, 197, 768]               0\n",
      "        LayerNorm-56              [1, 197, 768]           1,536\n",
      "           Linear-57             [1, 197, 3072]       2,362,368\n",
      "             GELU-58             [1, 197, 3072]               0\n",
      "          Dropout-59             [1, 197, 3072]               0\n",
      "         Identity-60             [1, 197, 3072]               0\n",
      "           Linear-61              [1, 197, 768]       2,360,064\n",
      "          Dropout-62              [1, 197, 768]               0\n",
      "              Mlp-63              [1, 197, 768]               0\n",
      "         Identity-64              [1, 197, 768]               0\n",
      "         Identity-65              [1, 197, 768]               0\n",
      "            Block-66              [1, 197, 768]               0\n",
      "        LayerNorm-67              [1, 197, 768]           1,536\n",
      "           Linear-68             [1, 197, 2304]       1,771,776\n",
      "         Identity-69           [1, 12, 197, 64]               0\n",
      "         Identity-70           [1, 12, 197, 64]               0\n",
      "           Linear-71              [1, 197, 768]         590,592\n",
      "          Dropout-72              [1, 197, 768]               0\n",
      "        Attention-73              [1, 197, 768]               0\n",
      "         Identity-74              [1, 197, 768]               0\n",
      "         Identity-75              [1, 197, 768]               0\n",
      "        LayerNorm-76              [1, 197, 768]           1,536\n",
      "           Linear-77             [1, 197, 3072]       2,362,368\n",
      "             GELU-78             [1, 197, 3072]               0\n",
      "          Dropout-79             [1, 197, 3072]               0\n",
      "         Identity-80             [1, 197, 3072]               0\n",
      "           Linear-81              [1, 197, 768]       2,360,064\n",
      "          Dropout-82              [1, 197, 768]               0\n",
      "              Mlp-83              [1, 197, 768]               0\n",
      "         Identity-84              [1, 197, 768]               0\n",
      "         Identity-85              [1, 197, 768]               0\n",
      "            Block-86              [1, 197, 768]               0\n",
      "        LayerNorm-87              [1, 197, 768]           1,536\n",
      "           Linear-88             [1, 197, 2304]       1,771,776\n",
      "         Identity-89           [1, 12, 197, 64]               0\n",
      "         Identity-90           [1, 12, 197, 64]               0\n",
      "           Linear-91              [1, 197, 768]         590,592\n",
      "          Dropout-92              [1, 197, 768]               0\n",
      "        Attention-93              [1, 197, 768]               0\n",
      "         Identity-94              [1, 197, 768]               0\n",
      "         Identity-95              [1, 197, 768]               0\n",
      "        LayerNorm-96              [1, 197, 768]           1,536\n",
      "           Linear-97             [1, 197, 3072]       2,362,368\n",
      "             GELU-98             [1, 197, 3072]               0\n",
      "          Dropout-99             [1, 197, 3072]               0\n",
      "        Identity-100             [1, 197, 3072]               0\n",
      "          Linear-101              [1, 197, 768]       2,360,064\n",
      "         Dropout-102              [1, 197, 768]               0\n",
      "             Mlp-103              [1, 197, 768]               0\n",
      "        Identity-104              [1, 197, 768]               0\n",
      "        Identity-105              [1, 197, 768]               0\n",
      "           Block-106              [1, 197, 768]               0\n",
      "       LayerNorm-107              [1, 197, 768]           1,536\n",
      "          Linear-108             [1, 197, 2304]       1,771,776\n",
      "        Identity-109           [1, 12, 197, 64]               0\n",
      "        Identity-110           [1, 12, 197, 64]               0\n",
      "          Linear-111              [1, 197, 768]         590,592\n",
      "         Dropout-112              [1, 197, 768]               0\n",
      "       Attention-113              [1, 197, 768]               0\n",
      "        Identity-114              [1, 197, 768]               0\n",
      "        Identity-115              [1, 197, 768]               0\n",
      "       LayerNorm-116              [1, 197, 768]           1,536\n",
      "          Linear-117             [1, 197, 3072]       2,362,368\n",
      "            GELU-118             [1, 197, 3072]               0\n",
      "         Dropout-119             [1, 197, 3072]               0\n",
      "        Identity-120             [1, 197, 3072]               0\n",
      "          Linear-121              [1, 197, 768]       2,360,064\n",
      "         Dropout-122              [1, 197, 768]               0\n",
      "             Mlp-123              [1, 197, 768]               0\n",
      "        Identity-124              [1, 197, 768]               0\n",
      "        Identity-125              [1, 197, 768]               0\n",
      "           Block-126              [1, 197, 768]               0\n",
      "       LayerNorm-127              [1, 197, 768]           1,536\n",
      "          Linear-128             [1, 197, 2304]       1,771,776\n",
      "        Identity-129           [1, 12, 197, 64]               0\n",
      "        Identity-130           [1, 12, 197, 64]               0\n",
      "          Linear-131              [1, 197, 768]         590,592\n",
      "         Dropout-132              [1, 197, 768]               0\n",
      "       Attention-133              [1, 197, 768]               0\n",
      "        Identity-134              [1, 197, 768]               0\n",
      "        Identity-135              [1, 197, 768]               0\n",
      "       LayerNorm-136              [1, 197, 768]           1,536\n",
      "          Linear-137             [1, 197, 3072]       2,362,368\n",
      "            GELU-138             [1, 197, 3072]               0\n",
      "         Dropout-139             [1, 197, 3072]               0\n",
      "        Identity-140             [1, 197, 3072]               0\n",
      "          Linear-141              [1, 197, 768]       2,360,064\n",
      "         Dropout-142              [1, 197, 768]               0\n",
      "             Mlp-143              [1, 197, 768]               0\n",
      "        Identity-144              [1, 197, 768]               0\n",
      "        Identity-145              [1, 197, 768]               0\n",
      "           Block-146              [1, 197, 768]               0\n",
      "       LayerNorm-147              [1, 197, 768]           1,536\n",
      "          Linear-148             [1, 197, 2304]       1,771,776\n",
      "        Identity-149           [1, 12, 197, 64]               0\n",
      "        Identity-150           [1, 12, 197, 64]               0\n",
      "          Linear-151              [1, 197, 768]         590,592\n",
      "         Dropout-152              [1, 197, 768]               0\n",
      "       Attention-153              [1, 197, 768]               0\n",
      "        Identity-154              [1, 197, 768]               0\n",
      "        Identity-155              [1, 197, 768]               0\n",
      "       LayerNorm-156              [1, 197, 768]           1,536\n",
      "          Linear-157             [1, 197, 3072]       2,362,368\n",
      "            GELU-158             [1, 197, 3072]               0\n",
      "         Dropout-159             [1, 197, 3072]               0\n",
      "        Identity-160             [1, 197, 3072]               0\n",
      "          Linear-161              [1, 197, 768]       2,360,064\n",
      "         Dropout-162              [1, 197, 768]               0\n",
      "             Mlp-163              [1, 197, 768]               0\n",
      "        Identity-164              [1, 197, 768]               0\n",
      "        Identity-165              [1, 197, 768]               0\n",
      "           Block-166              [1, 197, 768]               0\n",
      "       LayerNorm-167              [1, 197, 768]           1,536\n",
      "          Linear-168             [1, 197, 2304]       1,771,776\n",
      "        Identity-169           [1, 12, 197, 64]               0\n",
      "        Identity-170           [1, 12, 197, 64]               0\n",
      "          Linear-171              [1, 197, 768]         590,592\n",
      "         Dropout-172              [1, 197, 768]               0\n",
      "       Attention-173              [1, 197, 768]               0\n",
      "        Identity-174              [1, 197, 768]               0\n",
      "        Identity-175              [1, 197, 768]               0\n",
      "       LayerNorm-176              [1, 197, 768]           1,536\n",
      "          Linear-177             [1, 197, 3072]       2,362,368\n",
      "            GELU-178             [1, 197, 3072]               0\n",
      "         Dropout-179             [1, 197, 3072]               0\n",
      "        Identity-180             [1, 197, 3072]               0\n",
      "          Linear-181              [1, 197, 768]       2,360,064\n",
      "         Dropout-182              [1, 197, 768]               0\n",
      "             Mlp-183              [1, 197, 768]               0\n",
      "        Identity-184              [1, 197, 768]               0\n",
      "        Identity-185              [1, 197, 768]               0\n",
      "           Block-186              [1, 197, 768]               0\n",
      "       LayerNorm-187              [1, 197, 768]           1,536\n",
      "          Linear-188             [1, 197, 2304]       1,771,776\n",
      "        Identity-189           [1, 12, 197, 64]               0\n",
      "        Identity-190           [1, 12, 197, 64]               0\n",
      "          Linear-191              [1, 197, 768]         590,592\n",
      "         Dropout-192              [1, 197, 768]               0\n",
      "       Attention-193              [1, 197, 768]               0\n",
      "        Identity-194              [1, 197, 768]               0\n",
      "        Identity-195              [1, 197, 768]               0\n",
      "       LayerNorm-196              [1, 197, 768]           1,536\n",
      "          Linear-197             [1, 197, 3072]       2,362,368\n",
      "            GELU-198             [1, 197, 3072]               0\n",
      "         Dropout-199             [1, 197, 3072]               0\n",
      "        Identity-200             [1, 197, 3072]               0\n",
      "          Linear-201              [1, 197, 768]       2,360,064\n",
      "         Dropout-202              [1, 197, 768]               0\n",
      "             Mlp-203              [1, 197, 768]               0\n",
      "        Identity-204              [1, 197, 768]               0\n",
      "        Identity-205              [1, 197, 768]               0\n",
      "           Block-206              [1, 197, 768]               0\n",
      "       LayerNorm-207              [1, 197, 768]           1,536\n",
      "          Linear-208             [1, 197, 2304]       1,771,776\n",
      "        Identity-209           [1, 12, 197, 64]               0\n",
      "        Identity-210           [1, 12, 197, 64]               0\n",
      "          Linear-211              [1, 197, 768]         590,592\n",
      "         Dropout-212              [1, 197, 768]               0\n",
      "       Attention-213              [1, 197, 768]               0\n",
      "        Identity-214              [1, 197, 768]               0\n",
      "        Identity-215              [1, 197, 768]               0\n",
      "       LayerNorm-216              [1, 197, 768]           1,536\n",
      "          Linear-217             [1, 197, 3072]       2,362,368\n",
      "            GELU-218             [1, 197, 3072]               0\n",
      "         Dropout-219             [1, 197, 3072]               0\n",
      "        Identity-220             [1, 197, 3072]               0\n",
      "          Linear-221              [1, 197, 768]       2,360,064\n",
      "         Dropout-222              [1, 197, 768]               0\n",
      "             Mlp-223              [1, 197, 768]               0\n",
      "        Identity-224              [1, 197, 768]               0\n",
      "        Identity-225              [1, 197, 768]               0\n",
      "           Block-226              [1, 197, 768]               0\n",
      "       LayerNorm-227              [1, 197, 768]           1,536\n",
      "          Linear-228             [1, 197, 2304]       1,771,776\n",
      "        Identity-229           [1, 12, 197, 64]               0\n",
      "        Identity-230           [1, 12, 197, 64]               0\n",
      "          Linear-231              [1, 197, 768]         590,592\n",
      "         Dropout-232              [1, 197, 768]               0\n",
      "       Attention-233              [1, 197, 768]               0\n",
      "        Identity-234              [1, 197, 768]               0\n",
      "        Identity-235              [1, 197, 768]               0\n",
      "       LayerNorm-236              [1, 197, 768]           1,536\n",
      "          Linear-237             [1, 197, 3072]       2,362,368\n",
      "            GELU-238             [1, 197, 3072]               0\n",
      "         Dropout-239             [1, 197, 3072]               0\n",
      "        Identity-240             [1, 197, 3072]               0\n",
      "          Linear-241              [1, 197, 768]       2,360,064\n",
      "         Dropout-242              [1, 197, 768]               0\n",
      "             Mlp-243              [1, 197, 768]               0\n",
      "        Identity-244              [1, 197, 768]               0\n",
      "        Identity-245              [1, 197, 768]               0\n",
      "           Block-246              [1, 197, 768]               0\n",
      "       LayerNorm-247              [1, 197, 768]           1,536\n",
      "        Identity-248                   [1, 768]               0\n",
      "         Dropout-249                   [1, 768]               0\n",
      "          Linear-250                  [1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 86,415,592\n",
      "Trainable params: 86,415,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 479.03\n",
      "Params size (MB): 329.65\n",
      "Estimated Total Size (MB): 809.26\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3,224,224), batch_size=1, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(x)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_hat\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      5\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m y_hat[:,\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(x)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_hat\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      5\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m y_hat[:,\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Work/github/IgorJanos/nndemo/.venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Work/github/IgorJanos/nndemo/.venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = torch.randn(size=(2,3,224,224))\n",
    "y_hat = model(x)\n",
    "print(y_hat.shape)\n",
    "\n",
    "y_hat = y_hat[:,0]\n",
    "print(y_hat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
